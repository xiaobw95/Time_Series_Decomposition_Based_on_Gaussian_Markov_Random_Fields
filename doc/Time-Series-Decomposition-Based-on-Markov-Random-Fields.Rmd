---
title: "Time Series Decomposition Based on Markov Random Fields"
author: "Bowen Xiao"
date: "May 16, 2018"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

```{r echo=FALSE}
load("~/Time_Series_Decomposition_Based_on_Gaussian_Markov_Random_Fields/.RData")
```

```{r message=FALSE}
library(rstan)
library(loo)
library(forecast)
```

# Rest of the paper

## Intrduction

The project is about Markov random fields, which is a locally adaptive nonparametric curve fitting method that operates within a fully Bayesian framework. The model assumes that each data point is generated independently with some parametirc models, and the parameters are follow a Markov random fields model, which could, according to the paper, provide a combination of local adaptation and global control. Specificly, after removing seasonal component, my trend model used for forecasting looks like this:

$$
\begin{aligned}[t]
y_j &\sim normal(\theta_j, \sigma);\\
\sigma&\sim exponential(1)\\
\theta_{j} &= \rho^1\theta_{j-1} + \delta_{j-1};\\
\theta_1 &= 5*sd(y)*\theta_0 + \bar{y};\\
\theta_0 &\sim normal(0, 1);\\
\rho^1&\sim beta(2,2)\\
\delta_{j} &= \rho^2\delta_{j-1} + \delta^1_{j-1};\\
\delta^1_j &\sim normal(0, 1);\\
\delta_1 &\sim normal(0,1);\\
\rho^2&\sim beta(2,2)\\
\end{aligned}
$$

where the trace of $\theta$ is trend component.

What I want to get in time series decomposition is a nonparametric trend component, which is the main difference with classical methods, like ARIMA. In other word, I will not assume the parametric form of the trend component, like linear trend. Furthermore, I compare the results with some classical parametric techniques, like ARIMA, and machine learning techniques, like RNN/LSTM, with regrard to forecasting. Metrics for model evaluation and comparison is MSE.


## Data Description

I apply GMRF to my time series data. Specificly, I have data of bike collision records in downtown Seattle from 01/2004 to 06/2017 and I summarize them by month, so that I have 162 bike collision counts, which is shown as following. I split the data into train set (the first 150 points) and forecasting set (the last 12 points).

```{r echo=FALSE}
plot(TS,ylab='Bike Collision Counts',lwd=2)
```


## Time series decomposition

I apply GMRF fitting into time series decomposition and forecasting. Firstly, I use GMRF to fit a smoothing line. I split the line by bandwidth of 12 (since I know the period is 12 month), and avarage the difference with the mean in each piece. So that I get the seasonal component. Secondly, I minus the raw data with seasonal component and do another GMRF fitting on it, which turns out to be the trend component. For the first fitting, $y$ is Poisson distributed because it is count variable, but for the second fitting I use normal distribution.

I try three priors in first fitting: Gaussian prior, Laplace prior and Horseshoe prior. There is no issue of divergence.

```{r echo=FALSE}
check_hmc_diagnostics(TT)
check_hmc_diagnostics(TTL)
check_hmc_diagnostics(TTH)
```

And I compare them based on LOO-PSIS-CV and WAIC.

```{r echo=FALSE}
library(knitr)
kable(compare(loo1,loo2,loo3))
kable(compare(waic1,waic2,waic3))
```

Both LOO-PSIS-CV and WAIC indicate model based on Laplace prior is the best. Thus, I will go on with this model.

The trace of $theta_1$ in the sampling can be shown as following.

```{r echo=FALSE,fig.height=3}
traceplot(TTL,pars=c('theta[1]'))
```

The overlapping lines show that the samples from 4 different chains are comming from one common distribution, or that, there is no violation for $theta_1$. The conclusion is also true for other parameters. And as we can see in the following, all the $\hat{R}s$ are close to 1. Effective sample size is also large enough.

```{r echo=FALSE,fig.height=3}
plot(TTL, plotfun = "ess")
plot(TTL, plotfun = "rhat")
```

95% credible interval of $\theta$ can be shown as following.

```{r echo=FALSE}
plot_trend1(theta=list(postmed=fitted$postmed[1:150],bci.lower=fitted$bci.lower[1:150], bci.upper=fitted$bci.upper[1:150]), obstype="poisson", obsvar=tts, xvar=1:150, main="first GMRF fitting", xlab="time", ylab="count")
```

In the second fitting, there is no divergence issue either.

```{r echo=FALSE}
check_hmc_diagnostics(TTp)
```


Similarly, here are the results of second fitting.

```{r echo=FALSE,fig.height=3}
plot(TTp, plotfun = "ess")
plot(TTp, plotfun = "rhat")
```

```{r echo=FALSE,fig.height=4}
traceplot(TTp,pars=c('theta[1]'))
```

```{r echo=FALSE,fig.height=4}
traceplot(TTp,pars=c('sigma'))
```

```{r echo=FALSE}
plot_trend1(theta=extract_theta(TTp,obstype='normal'),
            obstype="normal", obsvar=c(test$y-seasonal,TS[151:162]-seasonal[139:150]), xvar=1:162,
            main="second GMRF fitting", xlab="time", ylab="")
```

where the last 12 $\theta$s are simulated and will be used in forecasting. 

The above blue line is my trend component. After second fitting, my decomposition will look like the following.

```{r echo=FALSE,fig.height=3.5}
par(mfrow=c(3,1), mar=c(2,1.5,1.5,1), oma=c(2,2,0,0))
plot(trend,type='l')
plot(seasonal,type='l')
plot(random,type='l')
```

As a comparison, decomposition based on ARIMA looks like the following. Generally speaking, the seasonal part seems to be exactly the same, but my trend line is more smoothing. 

```{r echo=FALSE,fig.height=4}
par(mfrow=c(1,1))
plot(decompose(TS))
```

And decomposition based on `Prophet` looks like the following.

```{r fig.height=3, message=FALSE,echo=FALSE,}
library(prophet)
ph<-prophet(data.frame(y=tts, ds=seq(ISOdate(2004,1,1), by = "month", length.out = 150)))
pfuture <- make_future_dataframe(ph, periods = 12)
pforecast <- predict(ph, pfuture)
prophet_plot_components(ph, pforecast)
```

Then, I will go further and compare them with regard to forecasting the last 12 points. Seasonal part seems to be straightforward to use in forecasting. For trend part, I use posterior median of $theta$. And then I sum up the seasonal component with a linear extension of trend component as my prediction.

```{r echo=FALSE,warning=FALSE}
A1<-mean((theta1[151:160]+seasonal[139:150]-TS[151:162])^2)
A4<-mean((ari$mean-TS[151:162])^2)
A5<-mean((rnn-TS[151:162])^2)
Ap<-mean((pforecast$trend[151:162]+pforecast$seasonal[151:162]-TS[151:162])^2)
```

The results of the above forecasting strategy along with other model's prediction, like ARIMA and RNN/LSTM are shown as following. I also show the result of `Prophet`.

```{r echo=FALSE}
kable(data.frame(Method=c("AUTO-ARIMS","RNN/LSTM","Prophet","GMRF"),MSE=round(c(A4,A5,Ap,A1),1)))
```

```{r echo=FALSE,warning=FALSE}
plot(c(151:162),TS[151:162],ylim=c(10,75),type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ari$mean,ylim=c(10,75),col=2,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),rnn,ylim=c(10,75),col=3,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),pforecast$yhat[151:162],ylim=c(10,75),col=4,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),theta1[151:160]+seasonal[139:150],ylim=c(10,75),col=5,type='l',main='Performance of prediction',xlab='time',ylab='count')
legend('topright', legend=c('Observation','AUTO-ARIMA','RNN/LSTM','Prophet','GMRF'), col=c(1:5), lty=1, cex=0.8)
```

As is shown above, GMRF obviously outperforms the other methods. RNN/LSTM and `Prophet` make similar performances. They are well-used models in parctice, for example, `Prophet` is used in many applications across Facebook for producing reliable forecasts for planning and goal setting. The main reason that they are not competitive here could be the amount of data is really small, or that, the issue of overfitting.

## Discussion

### Conclusion

As is mentioned above, GMRF is a powerful nonparametric regression method which can also be applied into time series. It has a good chance to catch various characteristic features of interests, like  autocorrelation structure and periodic component. 

In fact, priors have a shrinkage effect, which can also be seen as a kind of regularization. when it comes to MLE, ridge regression is equivalent to Gaussian prior, and Lasso regression is equivalent to Laplace prior. Under Bayesian framework, we are more flexible to choose from different kinds of priors.

When it comes to model structure, GMRF is a realization of partial-pooling model. The connections between $\theta_i$ is limited by distance, which balances local adaptation and global control. A partial-pooling model is always a good idea, bacause it is more flexible than a fully-pooling model and more controllable than a non-pooling model. It is a tradeoff of bias and variance.

### Limitation

The main issue here is computational efficience. Bayesian method is time-consuming. And Horseshoe prior has a more extreme problem of it than Gaussian prior and Laplace prior. 

The idea is to never let a Bayesian model to deal with a too complicated function fitting. Alternatively, we could firstly remove some component to make the model simpler. Another idea is to leave a mangeable amount of data for Bayesian model and fit a freqentist model with the rest data. And then include the results of frequentist model as priors. Besides, A general idea of online learning and ensemble learning is also appealing to me.


## Reference

[1]Faulkner, James R.; Minin, Vladimir N. Locally Adaptive Smoothing with Markov Random Fields and Shrinkage Priors. Bayesian Anal. 13 (2018), no. 1, 225--252. doi:10.1214/17-BA1050. https://projecteuclid.org/euclid.ba/1487905413

# Original Computational Environment

```{r}
sessionInfo()
```
