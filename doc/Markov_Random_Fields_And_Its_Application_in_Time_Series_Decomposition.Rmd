---
title: "Markov Random Fields And Its Application in Time Series Decomposition"
author: "Bowen Xiao"
date: "May 16, 2018"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
load("~/2018-research-project/.RData")
```

```{r echo=FALSE,message=FALSE}
library(rstan)
library(loo)
library(mice)
library(Kendall)
library(trend)
library(forecast)
library(splines)
library(e1071)
```

## Intrduction

The project is about Markov random fields, which is a locally adaptive nonparametric curve fitting method that operates within a fully Bayesian framework. The model assumes that each data point is generated independently with some parametirc models, and the parameters are follow a Markov random fields model, which could, according to the paper, provide a combination of local adaptation and global control. Specificly, after removing seasonal component, my trend model used for forecasting looks like this:

$$
\begin{aligned}[t]
y_i &\sim normal(\rho\theta_i, \sigma);\\
\sigma&\sim exponential(1)\\
\rho&\sim normal(0,1)I_{[0,1]}(\rho)\\
\theta_{j} &= \delta_{j-1} + \theta_{j-1};\\
\delta_{j} &= \delta_{j-1} + \delta^1_{j-1};\\
\delta^1_i &\sim normal(0, 1);\\
\theta_1 &= 5*sd(log(y))*\theta_0 + log(\bar{y});\\
\theta_0 &\sim normal(0, 1);\\
\delta_1 &\sim normal(0,10);\\
\end{aligned}
$$

where the trace of $\theta$ is trend component.

The idea of Bayesian Markov random fields comes from the paper in the reference[1]. This project includes the following two parts, simulation study of Gaussian Markov random fileds (GMRF) fitting and time series decomposition based on Markov random fiels. For time series decomposition, since it is count variable, in the first fitting I replace $y_i \sim normal(\theta_i, sigma)$ with $y_i \sim Poisson(e^{\theta_i})$. And different priors for $\delta$ are compared in both two fitting. As is well known, Bayes methods treat parameters as random variables. consequently, what we get is posterior distribution of $\theta$. To compare with frequentist methods, I will simply use the median of $\theta$. 

What I want to get in time series decomposition is a nonparametric trend component, which is the main difference with classical methods, like ARIMA. In other word, I will not assume the parametric form of the trend component, like linear trend. Furthermore, I compare the results with some classical parametric techniques, like ARIMA, and machine learning techniques, like RNN/LSTM, with regrard to forecasting. Metrics for model evaluation and comparison is MSE.



## Data Description

### Local Smoothness 

First of all, I apply Bayesian Markov random fields into nonparametric fitting and compare it with polynomial regression and NW estimators. Data will be generated randomly: $X_i\sim^{iid} Uniform(0,1)$ and $Y_i=f(X_i)+\epsilon_i$, where $f$ is known, $\epsilon_i\sim^{iid}N(0,1)$ and $\epsilon s$ are independent with $Xs$. In this project, I consider two scenarios: $f(X_i)=2X_i$ and $f(X_i)=sin(10X_i)$.

### Time Series Decomposition

Secondly, I apply this techinique to my time series data. Specificly, I have data of bike collision records in downtown Seattle from 01/2004 to 06/2017 and I summarize them by month, so that I have 162 bike collision counts, which is shown as following. I split the data into train set (the first 150 points) and test set (the last 12 points). 

```{r echo=FALSE}
plot(TS,ylab='Bike Collision Counts',lwd=2)
```

## Simulation

I run the full simulation study for  each of the 2 functions $f(x)$, and I calculate the MSE for $n = 100,200, \ldots, 500$ and for each $n$, we report the average MSE for each $n$, averaged over 10 replications of the data. 

My choice of bandwidth for NW estimators is motivated by the *optimal* bandwidth, which states that the optimal bandwidth should be $h_* = Cn^{-1/5}$ where the constant $C$ depends on the function $f$, variance of noise, and density of $x_i$. In this simulation, to keep things simple, we just take $C = 1$.

```{r message=FALSE,echo=FALSE,fig.height=3.5}
library(ggplot2)
ggplot(resA, mapping = aes(x = n, y = MSE, color = Method)) +
  geom_line() + facet_wrap(~Scenario, scales = "free") + scale_y_log10()
ggplot(resB, mapping = aes(x = n, y = MSE, color = Method)) +
  geom_line() + facet_wrap(~Scenario, scales = "free") + scale_y_log10()
```

As is shown,  GMRF fitting performs worst in the linear scenario but appeals to have some advantages in nonlinear scenario. Actually, the idea GMRF is very similar with some well-used nonparametric estimators, like KNN and NW estimators, because they all focus on local information. The difference is GMRF always treats $\{x_i,y_i\}$ as a sequence and it seems to be more flexible with the regularization assumptions of $f$, which is why I consider to apply it into time series. The referenced paper shows plenty of simulation study, including smoothing functions and piecewise constant function[1]. One example of my fitting is shown as following.

```{r message=FALSE,echo=FALSE,fig.height=4.5}
plot(x,y,col='grey',xlim=c(0,1),ylim=c(-4,4),ylab='y')
par(new=TRUE)
plot(x,sin(10*x),type='l',lwd=2,col=1,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh8,col=2,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh3,col=3,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh6,col=4,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh7,col=5,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
legend('topright', legend=c('TRUE','GMRF',"Poly Deg: 3", "NW-Box","NW-Gaussian"), col=c(1:5), lty=1, cex=0.8)
```


## Time series decomposition

I apply GMRF fitting into time series decomposition and forecasting. Firstly, I use GMRF to fit a smoothing line. I split the line by bandwidth of 12 (since I know the period is 12 month), and avarage the difference with the mean in each piece. So that I get the seasonal component. Secondly, I minus the raw data with seasonal component and do another GMRF fitting on it, which turns out to be the trend component. For the first fitting, $y$ is Poisson distributed because it is count variable, but for the second fitting I use normal distribution.

I try three priors in first fitting: Gaussian prior, Laplace prior and Horseshoe prior. There is no issue of divergence. And I compare them based on LOO-PSIS-CV (efficient approximate leave-one-out cross-validation for Bayesian models fit using Markov chain Monte Carlo, which uses Pareto smoothed importance sampling, a new procedure for regularizing importance weights.) and WAIC (a generalized version of AIC)[2].

```{r echo=FALSE}
library(knitr)
kable(compare(loo1,loo2,loo3))
kable(compare(waic1,waic2,waic3))
```

Both LOO-PSIS-CV and WAIC indicate model based on Laplace prior is the best. Thus, I will go on with this model.

The trace of $theta_1$ in the sampling can be shown as following.

```{r echo=FALSE,fig.height=3}
traceplot(TTL,pars=c('theta[1]'))
```

The overlapping lines show that the samples from 4 different chains are comming from one common distribution, or that, there is no violation for $theta_1$. The conclusion is also true for other parameters, because as we can see in the following, all the $\hat{R}s$ are close to 1 (The degree of convergence of a random Markov Chain can be estimated using the Gelman-Rubin convergence statistic, 
$\hat{R}$, based on the stability of outcomes between and within m chains of the same length, n. Values close to one indicate convergence to the underlying distribution. Values greater than 1.1 indicate inadequate convergence.)[3].

```{r echo=FALSE,fig.height=3}
plot(TTL, plotfun = "rhat")
```

95% credible interval of $\theta$ can be shown as following.

```{r echo=FALSE}
plot_trend1(theta=list(postmed=fitted$postmed[1:150],bci.lower=fitted$bci.lower[1:150], bci.upper=fitted$bci.upper[1:150]), obstype="poisson", obsvar=tts, xvar=1:150, main="first GMRF fitting", xlab="time", ylab="count")
```

In the second fitting, there is no divergence issue either. Similarly, here are the results of second fitting.

```{r echo=FALSE,fig.height=3}
plot(TTp, plotfun = "rhat")
```

```{r echo=FALSE,fig.height=4}
traceplot(TTp,pars=c('theta[1]'))
```

```{r echo=FALSE,fig.height=4}
traceplot(TTp,pars=c('sigma'))
```

```{r echo=FALSE}
plot_trend1(theta=extract_theta(TTp,obstype='normal'),
            obstype="normal", obsvar=c(test$y-seasonal,TS[151:162]-seasonal[139:150]), xvar=1:162,
            main="second GMRF fitting", xlab="time", ylab="")
```

where the last 12 $\theta$s are simulated and will be used in forecasting. 

The above blue line is my trend component. After second fitting, my decomposition will look like the following.

```{r echo=FALSE,fig.height=3.5}
par(mfrow=c(3,1), mar=c(2,1.5,1.5,1), oma=c(2,2,0,0))
plot(trend,type='l')
plot(seasonal,type='l')
plot(random,type='l')
```

As a comparison, decomposition based on ARIMA looks like the following. Generally speaking, the seasonal part seems to be exactly the same, but my trend line is more smoothing. 

```{r echo=FALSE,fig.height=4}
par(mfrow=c(1,1))
plot(decompose(TS))
```

And decomposition based on `Prophet` looks like the following.

```{r fig.height=3, message=FALSE,echo=FALSE,}
library(prophet)
ph<-prophet(data.frame(y=tts, ds=seq(ISOdate(2004,1,1), by = "month", length.out = 150)))
pfuture <- make_future_dataframe(ph, periods = 12)
pforecast <- predict(ph, pfuture)
prophet_plot_components(ph, pforecast)
```


Then, I will go further and compare them with regard to forecasting the last 12 points. Seasonal part seems to be straightforward to use in forecasting. For trend part, I use posterior median of $theta$. And then I sum up the seasonal component with a linear extension of trend component as my prediction.

```{r echo=FALSE,warning=FALSE}
A1<-mean((theta1[151:160]+seasonal[139:150]-TS[151:162])^2)
A4<-mean((ari$mean-TS[151:162])^2)
A5<-mean((rnn-TS[151:162])^2)
Ap<-mean((pforecast$trend[151:162]+pforecast$seasonal[151:162]-TS[151:162])^2)
```

The results of the above forecasting strategy along with other model's prediction, like ARIMA and RNN/LSTM are shown as following. I also show the result of `Prophet`.

```{r echo=FALSE}
kable(data.frame(Method=c("AUTO-ARIMS","RNN/LSTM","Prophet","GMRF"),MSE=round(c(A4,A5,Ap,A1),1)))
```

```{r echo=FALSE,warning=FALSE}
plot(c(151:162),TS[151:162],ylim=c(10,75),type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ari$mean,ylim=c(10,75),col=2,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),rnn,ylim=c(10,75),col=3,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),pforecast$yhat[151:162],ylim=c(10,75),col=4,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),theta1[151:160]+seasonal[139:150],ylim=c(10,75),col=5,type='l',main='Performance of prediction',xlab='time',ylab='count')
legend('topright', legend=c('Observation','AUTO-ARIMA','RNN/LSTM','Prophet','GMRF'), col=c(1:5), lty=1, cex=0.8)
```

As is shown above, GMRF obviously outperforms the other methods. RNN/LSTM and `Prophet` make similar performances. They are well-used models in parctice, for example, `Prophet` is used in many applications across Facebook for producing reliable forecasts for planning and goal setting. The main reason that they are not competitive here could be the amount of data is really small, or that, the issue of overfitting.

## Discussion

### Conclusion

As is mentioned above, GMRF is a powerful nonparametric regression method which can also be applied into time series. It has a good chance to catch various characteristic features of interests, like  autocorrelation structure and periodic component.

In fact, priors have a shrinkage effect, which can also be seen as a kind of regularization. when it comes to MLE, ridge regression is equivalent to Gaussian prior, and Lasso regression is equivalent to Laplace prior. Under Bayesian framework, we are more flexible to choose from different kinds of priors.

When it comes to model structure, GMRF is a realization of partial-pooling model. The connections between $\theta_i$ is limited by distance, which balances local adaptation and global control. A partial-pooling model is always a good idea, bacause it is more flexible than a fully-pooling model and more controllable than a non-pooling model. It is a tradeoff of bias and variance.

### Limitation

The main issue here is computational efficience. Bayesian method is time-consuming. In simulation study, for example, it takes totally about an hour to sample. And Horseshoe prior has a more extreme problem of it than Gaussian prior and Laplace prior. Moreover, if we try funtions like $y=sin(100x)$, divergence issues raise up. 

The idea is to never let a Bayesian model to deal with a too complicated function fitting. Alternatively, we could firstly remove some component to make the model simpler. Another idea is to leave a mangeable amount of data for Bayesian model and fit a freqentist model with the rest data. And then include the results of frequentist model as priors. Besides, A general idea of online learning and ensemble learning is also appealing to me.


## Reference

[1]Faulkner, James R.; Minin, Vladimir N. Locally Adaptive Smoothing with Markov Random Fields and Shrinkage Priors. Bayesian Anal. 13 (2018), no. 1, 225--252. doi:10.1214/17-BA1050. https://projecteuclid.org/euclid.ba/1487905413

[2]Vehtari, A., Gelman, A., and Gabry, J. (2017a). Practical Bayesian model evaluation using leaveone-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413â€“1432. doi:10.1007/s11222-
016-9696-4. (published version, arXiv preprint)

[3]Gelman, A. and D. B. Rubin (1992) Inference from iterative simulation using multiple sequences (with discussion). Statistical Science, 7:457-511
