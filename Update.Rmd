---
title: "Time Series Decomposition Based on Markov Random Fields"
author: "Bowen Xiao"
date: "May 16, 2018"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
load(".RData")
```

```{r echo=FALSE,message=FALSE}
library(rstan)
library(loo)
library(mice)
library(Kendall)
library(trend)
library(forecast)
library(splines)
library(e1071)
```

## Intrduction

The project is about Markov random fields, which is a locally adaptive nonparametric curve fitting method that operates within a fully Bayesian framework. The model assumes that each data point is generated independently with some parametirc models, like normal distributions, and the parameters are follow a Markov random fields model, which could, according to the paper, provide a combination of local adaptation and global control. For example, supposed $Y_i$ has already been ordered by $X_i$, then $Y_i\sim N(\theta_i,\sigma^2)$, where $i=1,2,...,n$, and $(\theta_{i+1}-\theta_i)|\tau_i\sim N(0,\tau_i^2)$, where $i=1,2,...,n-1$, and $\tau_i$ are independent and identically distributed. $\sigma^2$ is a global parameter and has its own prior. This is a nonparametric model because the number of parameters is increasing with the number of data points. Specificly, my model will look like this:

$$
\begin{aligned}[t]
zsigma &\sim uniform(0,1); \\
zgam &\sim uniform(0, 1);\\
ztheta1 &\sim normal(0, 1);\\
zdelta &\sim normal(0, 1);\\
sigma &= 5.0*tan(zsigma*\pi/2);\\
gam &= 0.06*tan(zgam*\pi/2);\\
theta_1 &= 5*sd(y)*ztheta1 + \bar{y};\\
theta_{j+1} &= gam*zdelta_j + theta_j;\\
y_i &\sim normal(theta_i, sigma);
\end{aligned}
$$

The idea of Bayesian Markov random fields comes from the paper in the reference[1]. This project includes the following two parts, simulation study of Gaussian Markov random fileds (GMRF) fitting and time series decomposition based on Markov random fiels. For time series decomposition, since it is count variable, in the first fitting I replace $y_i \sim normal(theta_i, sigma)$ with $y_i \sim Poisson(e^{theta_i}, sigma)$. As is well known, Bayes methods treat parameters as random variables. consequently, what we get is posterior distribution of $\theta$. To compare with frequentist methods, I will simply use the median of $\theta$. Furthermore, I choose the above non-centered parameterization to achieve better sampling performance, because centered parameterized model is found to have divergence issue in simulation study. 

## Data Description

### Local Smoothness 

First of all, I apply Bayesian Markov random fields into nonparametric fitting and compare it with polynomial regression and NW estimators. Data will be generated randomly: $X_i\sim^{iid} Uniform(0,1)$ and $Y_i=f(X_i)+\epsilon_i$, where $f$ is known, $\epsilon_i\sim^{iid}N(0,1)$ and $\epsilon s$ are independent with $Xs$. In this project, I consider two scenarios: $f(X_i)=2X_i$ and $f(X_i)=sin(10X_i)$.

### Time Series Decomposition

Secondly, I apply this techinique to my time series data. Specificly, I have data of bike collision records in downtown Seattle from 01/2004 to 06/2017 and I summarize them by month, so that I have 162 bike collision counts, which is shown as following. I decomposite this time series into trend part, seasonal part and random part. I treat it as an additive model and decomposit it by doing Markov random fields fitting twice. Furthermore, I compare the results with some classical parametric techniques, like ARIMA, and machine learning techniques, like RNN/LSTM, with regrard to forcasting.

```{r echo=FALSE}
plot(TS,ylab='Bike Collision Counts',lwd=2)
```

## Simulation

I run the full simulation study for  each of the 2 functions $f(x)$, and I calculate the MSE for $n = 100,200, \ldots, 500$ and for each $n$, we report the average MSE for each $n$, averaged over 10 replications of the data. 

My choice of bandwidth for NW estimators is motivated by the *optimal* bandwidth, which states that the optimal bandwidth should be $h_* = Cn^{-1/5}$ where the constant $C$ depends on the function $f$, variance of noise, and density of $x_i$. In this simulation, to keep things simple, we just take $C = 1$.

```{r message=FALSE,echo=FALSE,fig.height=3.5}
library(ggplot2)
ggplot(resA, mapping = aes(x = n, y = MSE, color = Method)) +
  geom_line() + facet_wrap(~Scenario, scales = "free") + scale_y_log10()
ggplot(resB, mapping = aes(x = n, y = MSE, color = Method)) +
  geom_line() + facet_wrap(~Scenario, scales = "free") + scale_y_log10()
```

As is shown,  GMRF fitting performs worst in the linear scenario but appeals to have some advantages in nonlinear scenario. Actually, the idea GMRF is very similar with some well-used nonparametric estimators, like KNN and NW estimators, because they all focus on local information. The difference is GMRF always treats $\{x_i,y_i\}$ as a sequence and it seems to be more flexible with the regularization assumptions of $f$, which is why I consider to apply it into time series. The referenced paper shows plenty of simulation study, including smoothing functions and piecewise constant function[1]. One example of my fitting is shown as following.

```{r message=FALSE,echo=FALSE,fig.height=4.5}
plot(x,y,col='grey',xlim=c(0,1),ylim=c(-4,4),ylab='y')
par(new=TRUE)
plot(x,sin(10*x),type='l',lwd=2,col=1,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh8,col=2,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh3,col=3,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh6,col=4,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh7,col=5,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
legend('topright', legend=c('TRUE','GMRF',"Poly Deg: 3", "NW-Box","NW-Gaussian"), col=c(1:5), lty=1, cex=0.8)
```


## Time series decomposition

I apply GMRF fitting into time series decomposition and forcasting. Firstly, I use GMRF to fit a smoothing line. I split the line by bandwidth of 12 (since I know the period is 12 month), and avarage the difference with the mean in each piece. So that I get the seasonal component. Secondly, I minus the raw data with seasonal component and do another GMRF fitting on it, which turns out to be the trend component. For the first fitting, $y$ is Poisson distributed because it is count variable, but for the second fitting I use normal distribution.

I try three priors in first fitting: Gaussian prior, Laplace prior and Horseshoe prior. Horseshoe has 2.08% divergences, while Gaussian prior and Laplace Prior show no problems of sampling. And I compare them based on LOO-PSIS-CV (efficient approximate leave-one-out cross-validation for Bayesian models fit using Markov chain Monte Carlo, which uses Pareto smoothed importance sampling, a new procedure for regularizing importance weights.) and WAIC (a generalized version of AIC)[2].

```{r echo=FALSE}
library(knitr)
kable(compare(loo1,loo2,loo3))
kable(compare(waic1,waic2,waic3))
```

Both LOO-PSIS-CV and WAIC shows model based on Gaussian prior is the best. Thus, I focus on GMRF in the following.

The trace of $theta_1$ in the sampling can be shown as following.

```{r echo=FALSE,fig.height=3}
traceplot(TT,pars=c('theta[1]'))
```

The overlapping lines show that the samples from 4 different chains are comming from one common distribution, or that, there is no violation for $theta_1$. The conclusion is also true for other parameters, because as we can see in the following, all the $\hat{R}s$ are close to 1 (The degree of convergence of a random Markov Chain can be estimated using the Gelman-Rubin convergence statistic, 
$\hat{R}$, based on the stability of outcomes between and within m chains of the same length, n. Values close to one indicate convergence to the underlying distribution. Values greater than 1.1 indicate inadequate convergence.)[3].

```{r echo=FALSE,fig.height=3}
plot(TT, plotfun = "rhat")
```

95% credible interval of $\theta$ can be shown as following.

```{r echo=FALSE}
plot_trend1(theta=extract_theta(TT,obstype='poisson'), obstype="poisson", obsvar=TS, xvar=1:162, main="first GMRF fitting", xlab="time", ylab="count")
```

The last 12 points is simulated. As we can see, using these simulated data points to make prediction is not a wise choice. So the second fitting is necessary.

Similarly, here are the results of second fitting.

```{r echo=FALSE,fig.height=3}
plot(TT1, plotfun = "rhat")
```

```{r echo=FALSE,fig.height=4}
traceplot(TT1,pars=c('theta[1]'))
```

```{r echo=FALSE,fig.height=4}
traceplot(TT1,pars=c('sigma'))
```



```{r echo=FALSE}
plot_trend1(theta=extract_theta(TT1,obstype='normal'), obstype="normal", obsvar=test$y-seasonal, xvar=1:150, main="second GMRF fitting", xlab="time", ylab="")
```


After second fitting, my decomposition will look like the following.

```{r echo=FALSE,fig.height=3.5}
par(mfrow=c(3,1), mar=c(2,1.5,1.5,1), oma=c(2,2,0,0))
plot(trend,type='l')
plot(seasonal,type='l')
plot(random,type='l')
```

As a comparison, decomposition based on ARIMA looks like the following.

```{r echo=FALSE,fig.height=5}
par(mfrow=c(1,1))
plot(decompose(TS))
```

Generally speaking, the seasonal part seems to be exactly the same, but my trend line is more smoothing. I will go further and compare them with regard to forcasting the last 12 points.

Seasonal part seems to be straightforward to use in forcasting. For trend part, firstly I consider a linear extension of trend component. The linear extension is implemented by `lm` function with a step wise chosen by cross validation, that is, I use the last few points to fit a linear regression to predict the future. Secondly, I also try to fit the trend line by B-spline or SVM. And then I sum up the seasonal component with a linear extension of trend component as my prediction.

```{r echo=FALSE,warning=FALSE}
A0<-mean((theta[151:162]-TS[151:162])^2)
A1<-mean((predict(lm(y~.,data=data.frame(y=trend[(150-step_width+1):150],x=c((150-step_width+1):150))),
              newdata=data.frame(x=c(151:162)))+seasonal[139:150]-TS[151:162])^2)
A2<-mean((seasonal[139:150]+predict(long_line1,newdata = data.frame(x=c(151:162)))-TS[151:162])^2)
A3<-mean((predYsvm+seasonal[139:150]-TS[151:162])^2)
A4<-mean((ari$mean-TS[151:162])^2)
A5<-mean((rnn-TS[151:162])^2)
```

The results of the above forcasting strategy along with other model's prediction, like ARIMA and RNN/LSTM are shown as following.

```{r echo=FALSE}
kable(data.frame(Method=c("AUTO-ARIMS","RNN/LSTM","GMRF-Simulation","GMRF+LinearExtension","GMRF+BSpline","GMRF+SVM"),MSE=round(c(A4,A5,A0,A1,A2,A3),1)))
```

```{r echo=FALSE,warning=FALSE}
plot(c(151:162),TS[151:162],ylim=c(10,75),type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ari$mean,ylim=c(10,75),col=2,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),rnn,ylim=c(10,75),col=3,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ylim=c(10,75),predict(lm(y~.,data=data.frame(y=trend[(150-step_width+1):150],x=c((150-step_width+1):150))),newdata=data.frame(x=c(151:162)))+seasonal[139:150],col=4,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ylim=c(10,75),seasonal[139:150]+predict(long_line1,newdata = data.frame(x=c(151:162))),col=5,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ylim=c(10,75),predYsvm+seasonal[139:150],col=6,type='l',main='Performance of prediction',xlab='time',ylab='count')
legend('topright', legend=c('Observation','AUTO-ARIMA','RNN/LSTM','GMRF+LE','GMRF+BS','GMRF+SVM'), col=c(1:6), lty=1, cex=0.8)
```

As is shown above, GMRF obviously outperforms the other methods, and using B-splines to fit the trend line seems to be the best choice.


## Discussion

### Conclusion

As is mentioned above, GMRF is a powerful nonparametric regression method which can also be applied into time series. It has a good chance to catch various characteristic features of interests, like  autocorrelation structure and periodic component. Combining GMRF and B-splines achieves really nice results, which is beyond my proposal. 

In fact, priors have a shrinkage effect, which can also be seen as a kind of regularization. when it comes to MLE, ridge regression is equivalent to Gaussian prior, and Lasso regression is equivalent to Laplace prior. Under Bayesian framework, we are more flexible to choose from different kinds of priors.

When it comes to model structure, GMRF is a realization of partial-pooling model. The connections between $\theta_i$ is limited by distance, which balances local adaptation and global control. A partial-pooling model is always a good idea, bacause it is more flexible than a fully-pooling model and more controllable than a non-pooling model. It is a tradeoff of bias and variance.

### Limitation

The main issue here is computational efficience. Bayesian method is time-consuming. In simulation study, for example, it takes totally about an hour to sample. And Horseshoe prior has a more extreme problem of it than Gaussian prior and Laplace prior. Moreover, if we try funtions like $y=sin(100x)$, divergence issues raise up. 

My idea is to take advantages of both fitting power of Bayesian frameworks and computational efficience of frequentist methos. firstly, we should never let a Bayesian model to deal with a too complicated function fitting. Alternatively, we may use method like B-splines with knots to do a quick pilot fitting, and then run a divide and conquer algorithm with Bayesian model. Secondly, a Bayesian method could be a correction to a frequentist method, and vice versa. Actually, `GMRF+BSpline` is an example of realization. Another idea is to leave a mangeable amount of data for Bayesian model and fit a freqentist model with the rest data. And then include the results of frequentist model as priors. Besides, A general idea of ensemble learning is also appealing to me.


## Reference

[1]Faulkner, James R.; Minin, Vladimir N. Locally Adaptive Smoothing with Markov Random Fields and Shrinkage Priors. Bayesian Anal. 13 (2018), no. 1, 225--252. doi:10.1214/17-BA1050. https://projecteuclid.org/euclid.ba/1487905413

[2]Vehtari, A., Gelman, A., and Gabry, J. (2017a). Practical Bayesian model evaluation using leaveone-out cross-validation and WAIC. Statistics and Computing. 27(5), 1413â€“1432. doi:10.1007/s11222-
016-9696-4. (published version, arXiv preprint)

[3]Gelman, A. and D. B. Rubin (1992) Inference from iterative simulation using multiple sequences (with discussion). Statistical Science, 7:457-511
