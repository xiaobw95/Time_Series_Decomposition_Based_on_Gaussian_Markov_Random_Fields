---
title: "Time Series Decomposition Based on Markov Random Fields"
author: "Bowen Xiao"
date: "May 16, 2018"
output:
  pdf_document:
    latex_engine: xelatex
  html_document: default
linkcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r echo=FALSE}
load(".RData")
```

```{r echo=FALSE,message=FALSE}
library(rstan)
library(loo)
library(mice)
library(Kendall)
library(trend)
library(forecast)
library(splines)
library(e1071)
```

## Intrduction

The project is about Markov random fields, which is a locally adaptive nonparametric curve fitting method that operates within a fully Bayesian framework. The model assumes that each data point is generated independently with some parametirc models, like normal distributions, and the parameters are follow a Markov random fields model, which could, according to the paper, provide a combination of local adaptation and global control. For example, supposed $Y_i$ has already been ordered by $X_i$, then $Y_i\sim N(\theta_i,\sigma^2)$, where $i=1,2,...,n$, and $(\theta_{i+1}-\theta_i)|\tau_i\sim N(0,\tau_i^2)$, where $i=1,2,...,n-1$, and $\tau_i$ are independent and identically distributed. $\sigma^2$ is a global parameter and has its own prior. This is a nonparametric model because the number of parameters is increasing with the number of data points. Specificly, my model will look like this:

$$
\begin{aligned}[t]
zsigma &\sim uniform(0,1); \\
zgam &\sim uniform(0, 1);\\
ztheta1 &\sim normal(0, 1);\\
zdelta &\sim normal(0, 1);\\
sigma &= 5.0*tan(zsigma*pi()/2);\\
gam &= 0.06*tan(zgam*pi()/2);\\
theta_1 &= 5*sd(y)*ztheta1 + \bar{y};\\
theta_{j+1} &= gam*zdelta_j + theta_j;\\
y_i &\sim normal(theta_i, sigma);
\end{aligned}
$$

I choose the above non-centered parameterization to achieve better sampling performance, because centered parameterized model draws bad samples in simulation study. The idea of Bayesian Markov random fields comes from the paper in the reference. This project includes the following two parts, simulation study of Gaussian Markov random fileds (GMRF) fitting and time series decomposition based on Markov random fiels. 

## Data Description

### Local Smoothness 

First of all, I apply Bayesian Markov random fields into nonparametric fitting and compare it with polynomial regression and NW estimators. Data will be generated randomly: $X_i\sim^{iid} Uniform(0,1)$ and $Y_i=f(X_i)+\epsilon_i$, where $f$ is known, $\epsilon_i\sim^{iid}N(0,1)$ and $\epsilon s$ are independent with $Xs$. In this project, I consider two scenarios: $f(X_i)=2X_i$ and $f(X_i)=sin(10X_i)$.

### Time Series Decomposition

Secondly, I apply this techinique to my time series data. Specificly, I have data of bike collision records in downtown Seattle from 01/2004 to 06/2017 and I summarize them by month, so that I have 162 bike collision counts, which is shown as following. I decomposite this time series into trend part, seasonal part and random part. I treat it as an additive model and decomposit it by doing Markov random fields fitting twice. Furthermore, I compare the results with some classical parametric techniques, like ARIMA, and machine learning techniques, like RNN/LSTM, with regrard to forcasting.

```{r echo=FALSE}
plot(TS,ylab='Bike Collision Counts',lwd=2)
```

## Simulation

I run the full simulation study for  each of the 2 functions $f(x)$, and I calculate the MSE for $n = 100,200, \ldots, 500$ and for each $n$, we report the average MSE for each $n$, averaged over 10 replications of the data. 

My choice of bandwidth for NW estimators is motivated by the *optimal* bandwidth, which states that the optimal bandwidth should be $h_* = Cn^{-1/5}$ where the constant $C$ depends on the function $f$, variance of noise, and density of $x_i$. In this simulation, to keep things simple, we just take $C = 1$.

```{r message=FALSE,echo=FALSE}
library(ggplot2)
ggplot(resA, mapping = aes(x = n, y = MSE, color = Method)) +
  geom_line() + facet_wrap(~Scenario, scales = "free") + scale_y_log10()
ggplot(resB, mapping = aes(x = n, y = MSE, color = Method)) +
  geom_line() + facet_wrap(~Scenario, scales = "free") + scale_y_log10()
```

As is shown,  GMRF fitting performs worst in the linear scenario but appeals to have some advantages in nonlinear scenario. Actually, the idea GMRF is very similar with some well-used nonparametric estimators, like KNN and NW estimators, because they all localized the fitting. The difference is GMRF always treats $\{x_i,y_i\}$ as a sequence and it seems to be more flexible with the regularization assumptions of $f$, which is why I consider to apply it into time series. The referenced paper includes plenty of simulation study. One example fitting of mine is shown as following.

```{r message=FALSE,echo=FALSE}
plot(x,y,col='grey',xlim=c(0,1),ylim=c(-4,4),ylab='y')
par(new=TRUE)
plot(x,sin(10*x),type='l',lwd=2,col=1,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh8,col=2,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh3,col=3,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh6,col=4,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
par(new=TRUE)
plot(x,yh7,col=5,type='l',lwd=2,xlim=c(0,1),ylim=c(-4,4),main='y = sin(10x)',ylab='y')
legend('topright', legend=c('TRUE','GMRF',"Poly Deg: 3", "NW-Box","NW-Gaussian"), col=c(1:5), lty=1, cex=0.8)
```


## Time series decomposition

I apply GMRF fitting into time series decomposition and forcasting. Firstly, I use GMRF to fit a smoothing line. I split the line by bandwidth of 12 (since I know the period is 12 month), and avarage the difference with the mean in each piece. So that I get the seasonal component. Secondly, I minus the raw data with seasonal component and do another GMRF fitting on it, which turns out to be the trend component. For the first fitting, $y$ is Poisson distributed because it is count variable, but for the second fitting I use normal distribution.

I try three priors in first fitting: Gaussian prior, Laplace prior and Horseshoe prior. Horseshoe has 2.08% divergences, while Gaussian prior and Laplace Prior show no problems of sampling. And I compare them based on LOO-PSIS-CV and WAIC.

```{r echo=FALSE}
library(knitr)
kable(compare(loo1,loo2,loo3))
kable(compare(waic1,waic2,waic3))
```

Both LOO-PSIS-CV and WAIC shows model based on Gaussian prior is the best. Thus, I focus on GMRF in the following.

The trace of $theta_1$ in the sampling can be shown as following.

```{r echo=FALSE}
traceplot(TT,pars=c('theta[1]'))
```

The overlapping lines show that the samples from 4 different chains are comming from one common distribution, or that, there is no violation for $theta_1$. The conclusion is also true for other parameters, because as we can see in the following, all the $\hat{R}s$ are close to 1.

```{r echo=FALSE}
plot(TT, plotfun = "rhat")
```

95% credible interval of $log(E[y_i])$ can be shown as following.

```{r echo=FALSE}
plot(TT,pars=c('theta'))
```

The last 12 points is simulated. As we can see, using these simulated data points to make prediction is not a wise choice. So the second fitting is necessary.

After second fitting, my decomposition will look like the following.

```{r echo=FALSE}
par(mfrow=c(3,1), mar=c(2,1.5,1.5,1), oma=c(2,2,0,0))
plot(trend,type='l')
plot(seasonal,type='l')
plot(random,type='l')
```

As a comparison, decomposition based on ARIMA looks like the following.

```{r echo=FALSE}
par(mfrow=c(1,1))
plot(decompose(TS))
```

Generally speaking, the seasonal part seems to be exactly the same, but my trend line is more smoothing. I will go further and compare them with regard to forcasting the last 12 points.

Seasonal part seems to be straightforward to use in forcasting. For trend part, firstly I consider a linear extension of trend component. The linear extension is implemented by `lm` function with a step wise chosen by cross validation, that is, I use the last few points to fit a linear regression to predict the future. Secondly, I also try to fit the trend line by B-spline or SVM. And then I sum up the seasonal component with a linear extension of trend component as my prediction.

```{r echo=FALSE,warning=FALSE}
A1<-mean((predict(lm(y~.,data=data.frame(y=trend[(150-step_width+1):150],x=c((150-step_width+1):150))),
              newdata=data.frame(x=c(151:162)))+seasonal[139:150]-TS[151:162])^2)
A2<-mean((seasonal[139:150]+predict(long_line1,newdata = data.frame(x=c(151:162)))-TS[151:162])^2)
A3<-mean((predYsvm+seasonal[139:150]-TS[151:162])^2)
A4<-mean((ari$mean-TS[151:162])^2)
A5<-mean((rnn-TS[151:162])^2)
```

The results of the above forcasting strategy along with other model's prediction, like ARIMA and RNN/LSTM are shown as following.

```{r echo=FALSE,warning=FALSE}
plot(c(151:162),TS[151:162],ylim=c(10,75),type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ari$mean,ylim=c(10,75),col=2,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),rnn,ylim=c(10,75),col=3,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ylim=c(10,75),predict(lm(y~.,data=data.frame(y=trend[(150-step_width+1):150],x=c((150-step_width+1):150))),newdata=data.frame(x=c(151:162)))+seasonal[139:150],col=4,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ylim=c(10,75),seasonal[139:150]+predict(long_line1,newdata = data.frame(x=c(151:162))),col=5,type='l',main='Performance of prediction',xlab='time',ylab='count')
par(new=TRUE)
plot(c(151:162),ylim=c(10,75),predYsvm+seasonal[139:150],col=6,type='l',main='Performance of prediction',xlab='time',ylab='count')
legend('topright', legend=c('Observation','AUTO-ARIMA','RNN/LSTM','GMRF+LE','GMRF+BS','GMRF+SVM'), col=c(1:6), lty=1, cex=0.8)
```


```{r echo=FALSE}
kable(data.frame(Method=c("AUTO-ARIMS","RNN/LSTM","GMRF+LinearExtension","GMRF+BSpline","GMRF+SVM"),MSE=round(c(A4,A5,A1,A2,A3),1)))
```


## Discussion


## Reference

[1]Faulkner, James R.; Minin, Vladimir N. Locally Adaptive Smoothing with Markov Random Fields and Shrinkage Priors. Bayesian Anal. 13 (2018), no. 1, 225--252. doi:10.1214/17-BA1050. https://projecteuclid.org/euclid.ba/1487905413
